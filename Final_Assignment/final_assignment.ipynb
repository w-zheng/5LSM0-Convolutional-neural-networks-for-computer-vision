{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "detailed-david",
   "metadata": {},
   "source": [
    "# 5LSM0 Final Assignment </br>\n",
    " Cityscapes Image Quality/Generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understanding-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import shutil\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "import re\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import random\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import math\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "worst-syndicate",
   "metadata": {},
   "source": [
    "## Downloading and preprocessing the ground truth and input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-brand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directory. Change this to download to a different directory, e.g. to an external drive to save space. \n",
    "# You need 20 GB to store all data.\n",
    "# If you use Google Colab to run this notebook, then you may want to point this to a Google Drive directory shared\n",
    "# between you and your assignment partner.\n",
    "dir_data = os.path.abspath(\"data\")\n",
    "\n",
    "# URLs to retrieve ground truth and images data from. \n",
    "url_truth = 'https://flux127120.nbw.tue.nl/index.php/s/Cwxa5Ft2pQBK9N7/download'\n",
    "dir_truth = os.path.join(dir_data, \"gtFine\")\n",
    "\n",
    "url_input = 'https://flux127120.nbw.tue.nl/index.php/s/Tz3GCjQwwsiHgqC/download'\n",
    "dir_input = os.path.join(dir_data, \"leftImg8bit\")\n",
    "\n",
    "# Download and extraction function\n",
    "def download_extract(url: str):\n",
    "    # Create a temp directory to download into\n",
    "    with tempfile.TemporaryDirectory(dir=dir_data, prefix=\"download_\") as dir_temp:\n",
    "        print(f'Downloading: {url}')\n",
    "        zip_path = os.path.join(dir_temp, 'download.zip')\n",
    "        urlretrieve(url, zip_path, lambda n, size, total: sys.stdout.write(f'\\rProgress: {n*size/total*100:.2f} %'))\n",
    "        sys.stdout.write('\\n')\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        print(f'Unpacking archive.')\n",
    "        shutil.unpack_archive(zip_path, dir_data)\n",
    "\n",
    "# Create the data directory (if it does not exist)\n",
    "os.makedirs(dir_data, exist_ok=True)\n",
    "\n",
    "# Check if both the ground truth and input directories have been downloaded and extracted\n",
    "for dir, url in [(dir_truth, url_truth), (dir_input, url_input)]:\n",
    "    if not os.path.isdir(dir):\n",
    "        # Download the required files\n",
    "        print(f'Directory does not exist: {dir}')\n",
    "        download_extract(url)\n",
    "    else:\n",
    "        print(f'Directory already downloaded: {dir}')\n",
    "\n",
    "# Done!\n",
    "print(f'All data downloaded')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dramatic-analysis",
   "metadata": {},
   "source": [
    "## Downsampling of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-calculator",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Target size of each sample in the dataset\n",
    "sample_size = (512, 256)\n",
    "\n",
    "# Directories for preprocessed datasets\n",
    "dir_truth_pp, dir_input_pp = (f'{d}_{sample_size[0]}_{sample_size[1]}' for d in (dir_truth, dir_input))\n",
    "\n",
    "# Run preprocessing\n",
    "for dir_full, dir_pp in ((dir_truth, dir_truth_pp), (dir_input, dir_input_pp)):\n",
    "    # Check if the directory already exists\n",
    "    if os.path.isdir(dir_pp):\n",
    "        print(f'Preprocessed directory already exists: {dir_pp}')\n",
    "        continue\n",
    "\n",
    "    print(f'Preprocessing: {dir_full}')\n",
    "\n",
    "    # Walk though the directory and preprocess each file \n",
    "    for root,_,files in  os.walk( dir_full ):\n",
    "        if len(files) == 0:\n",
    "            continue\n",
    "\n",
    "        print(f'Preprocessing sub-directory: {root.replace(dir_full, \"\")}')\n",
    "\n",
    "        # Create the directory in the preprocessed set\n",
    "        root_pp = root.replace(dir_full, dir_pp)\n",
    "        os.makedirs(root_pp, exist_ok=True)\n",
    "\n",
    "        for f in files:\n",
    "            if not f.endswith('.png'):\n",
    "                continue\n",
    "\n",
    "            # Resize and save PNG image\n",
    "            path_original = os.path.join(root,f)\n",
    "            img_resized = Image.open(path_original).resize(sample_size, Image.NEAREST)\n",
    "            img_resized.save(path_original.replace(dir_full, dir_pp), 'png', quality=100)\n",
    "\n",
    "print(f'Preprocessing done')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24959217",
   "metadata": {},
   "source": [
    "## Merge rain and foggy data to original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8336c6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add image/label to the dataset\n",
    "def merge_data(root, dir):\n",
    "    new_name = \"_\".join(dir.split(\"_\")[:4])+\".png\"\n",
    "    type = root.split(\"\\\\\")[-2]\n",
    "    city = dir.split(\"_\")[:4][0]\n",
    "    paths = os.path.join(root, dir)\n",
    "    original_root = root.replace(\"_foggy\",\"\").replace(\"_rain\",\"\")\n",
    "    if os.path.exists(os.path.join(original_root.replace(\"leftImg8bit\", f\"leftImg8bit_{sample_size[0]}_{sample_size[1]}\"), new_name)):\n",
    "        img_resized = Image.open(paths).resize(sample_size, Image.NEAREST)\n",
    "        os.makedirs(os.path.join(dir_input_pp, type, city), exist_ok=True)\n",
    "        img_resized.save(os.path.join(dir_input_pp, type, city, new_name), \"png\", quality=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b216094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_data = input(\"Add external data of rain and fog? (Yes / No)\")\n",
    "if add_data==\"Yes\":\n",
    "    add=True\n",
    "else:\n",
    "    add=False\n",
    "    print(\"No data merged\")\n",
    "    pass\n",
    "\n",
    "# Set up the directory for rain and fog\n",
    "rain_data = os.path.join(dir_data, \"leftImg8bit_foggy\")\n",
    "fog_data = os.path.join(dir_data, \"leftImg8bit_rain\")\n",
    "\n",
    "# Number of external data\n",
    "num_external = 15\n",
    "\n",
    "# Addition of external data\n",
    "if add:\n",
    "    for dir_full in (fog_data, rain_data):\n",
    "        for root,_,files in os.walk(dir_full):\n",
    "            if len(files) == 0:\n",
    "                continue\n",
    "            # Choose random images for train/val/test\n",
    "            choice = random.choices(files, k=num_external)\n",
    "\n",
    "            for dir in choice:\n",
    "                merge_data(root, dir)\n",
    "\n",
    "    print(\"Data merge completed\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "several-vehicle",
   "metadata": {},
   "source": [
    "## Data structures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-doctrine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each sample we downloaded can be identified by the name of the city as well as a frame and sequence id\n",
    "@dataclass\n",
    "class CityscapesSample:\n",
    "    city: str\n",
    "    seq_id: str\n",
    "    frame_id: str\n",
    "\n",
    "    @property\n",
    "    def id(self):\n",
    "        return os.path.join(self.city, \"_\".join([self.city, self.seq_id, self.frame_id]))\n",
    "\n",
    "    @staticmethod\n",
    "    def from_filename(filename: str):\n",
    "        # Create a CityscapesSample from a filename, which has a fixed structure {city}_{sequence}_{frame}\n",
    "        match = re.match(r\"^(\\w+)_(\\d+)_(\\d+).*.png$\", filename, re.I)\n",
    "        return CityscapesSample(match.group(1), match.group(2), match.group(3))\n",
    "\n",
    "\n",
    "# Each class that we aim to detect is assigned a name, id and color.\n",
    "@dataclass\n",
    "class CityscapesClass:\n",
    "    name: str       # The identifier of this label, e.g. 'car', 'person', ... .\n",
    "                    # We use them to uniquely name a class\n",
    "\n",
    "    ID: int         # An integer ID that is associated with this label.\n",
    "                    # The IDs are used to represent the label in ground truth images\n",
    "                    # An ID of -1 means that this label does not have an ID and thus\n",
    "                    # is ignored when creating ground truth images (e.g. license plate).\n",
    "                    # Do not modify these IDs, since exactly these IDs are expected by the\n",
    "                    # evaluation server.\n",
    "\n",
    "    trainId: int    # Feel free to modify these IDs as suitable for your method. Then create\n",
    "                    # ground truth images with train IDs, using the tools provided in the\n",
    "                    # 'preparation' folder. However, make sure to validate or submit results\n",
    "                    # to our evaluation server using the regular IDs above!\n",
    "                    # For trainIds, multiple labels might have the same ID. Then, these labels\n",
    "                    # are mapped to the same class in the ground truth images. For the inverse\n",
    "                    # mapping, we use the label that is defined first in the list below.\n",
    "                    # For example, mapping all void-type classes to the same ID in training,\n",
    "                    # might make sense for some approaches.\n",
    "                    # Max value is 255!\n",
    "\n",
    "    category: str   # The name of the category that this label belongs to\n",
    "\n",
    "    categoryId: int # The ID of this category. Used to create ground truth images\n",
    "                    # on category level.\n",
    "\n",
    "    hasInstances: bool # Whether this label distinguishes between single instances or not\n",
    "\n",
    "    ignoreInEval: bool # Whether pixels having this class as ground truth label are ignored\n",
    "                       # during evaluations or not\n",
    "\n",
    "    color: Tuple[int, int, int]       # The color of this label\n",
    "\n",
    "\n",
    "# List of classes that we want to detect in the input\n",
    "classes = [\n",
    "    #                 name                     ID    trainId   category            catId     hasInstances   ignoreInEval   color\n",
    "    CityscapesClass(  'unlabeled'            ,  0 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    CityscapesClass(  'ego vehicle'          ,  1 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    CityscapesClass(  'rectification border' ,  2 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    CityscapesClass(  'out of roi'           ,  3 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    CityscapesClass(  'static'               ,  4 ,      255 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    CityscapesClass(  'dynamic'              ,  5 ,      255 , 'void'            , 0       , False        , True         , (111, 74,  0) ),\n",
    "    CityscapesClass(  'ground'               ,  6 ,      255 , 'void'            , 0       , False        , True         , ( 81,  0, 81) ),\n",
    "    CityscapesClass(  'road'                 ,  7 ,        0 , 'flat'            , 1       , False        , False        , (128, 64,128) ),\n",
    "    CityscapesClass(  'sidewalk'             ,  8 ,        1 , 'flat'            , 1       , False        , False        , (244, 35,232) ),\n",
    "    CityscapesClass(  'parking'              ,  9 ,      255 , 'flat'            , 1       , False        , True         , (250,170,160) ),\n",
    "    CityscapesClass(  'rail track'           , 10 ,      255 , 'flat'            , 1       , False        , True         , (230,150,140) ),\n",
    "    CityscapesClass(  'building'             , 11 ,        2 , 'construction'    , 2       , False        , False        , ( 70, 70, 70) ),\n",
    "    CityscapesClass(  'wall'                 , 12 ,        3 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n",
    "    CityscapesClass(  'fence'                , 13 ,        4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n",
    "    CityscapesClass(  'guard rail'           , 14 ,      255 , 'construction'    , 2       , False        , True         , (180,165,180) ),\n",
    "    CityscapesClass(  'bridge'               , 15 ,      255 , 'construction'    , 2       , False        , True         , (150,100,100) ),\n",
    "    CityscapesClass(  'tunnel'               , 16 ,      255 , 'construction'    , 2       , False        , True         , (150,120, 90) ),\n",
    "    CityscapesClass(  'pole'                 , 17 ,        5 , 'object'          , 3       , False        , False        , (153,153,153) ),\n",
    "    CityscapesClass(  'polegroup'            , 18 ,      255 , 'object'          , 3       , False        , True         , (  0,  0,  0) ),\n",
    "    CityscapesClass(  'traffic light'        , 19 ,        6 , 'object'          , 3       , False        , False        , (250,170, 30) ),\n",
    "    CityscapesClass(  'traffic sign'         , 20 ,        7 , 'object'          , 3       , False        , False        , (220,220,  0) ),\n",
    "    CityscapesClass(  'vegetation'           , 21 ,        8 , 'nature'          , 4       , False        , False        , (107,142, 35) ),\n",
    "    CityscapesClass(  'terrain'              , 22 ,        9 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n",
    "    CityscapesClass(  'sky'                  , 23 ,       10 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n",
    "    CityscapesClass(  'person'               , 24 ,       11 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n",
    "    CityscapesClass(  'rider'                , 25 ,       12 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n",
    "    CityscapesClass(  'car'                  , 26 ,       13 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n",
    "    CityscapesClass(  'truck'                , 27 ,       14 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n",
    "    CityscapesClass(  'bus'                  , 28 ,       15 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n",
    "    CityscapesClass(  'caravan'              , 29 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0, 90) ),\n",
    "    CityscapesClass(  'trailer'              , 30 ,      255 , 'vehicle'         , 7       , True         , True         , (  0,  0,110) ),\n",
    "    CityscapesClass(  'train'                , 31 ,       16 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n",
    "    CityscapesClass(  'motorcycle'           , 32 ,       17 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n",
    "    CityscapesClass(  'bicycle'              , 33 ,       18 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) ),\n",
    "    CityscapesClass(  'license plate'        , -1 ,      255 , 'vehicle'         , 7       , False        , True         , (  0,  0,  0) ),\n",
    "]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "negative-tuition",
   "metadata": {},
   "source": [
    "## Dataset implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-briefing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from typing import Dict, Optional, Tuple, List\n",
    "\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CityscapesDataset(Dataset):\n",
    "    # Regular expression matching each PNG file in the dataset\n",
    "    __read_reg = r\"^(\\w+)_(\\d+)_(\\d+).*.png$\"\n",
    "\n",
    "    def __init__(self, dir_input: str, dir_truth: str, sample_size: Tuple[int,int], classes: List[CityscapesSample]):\n",
    "        super().__init__()\n",
    "\n",
    "        # These variables are also available as globals, but it is good practice to make classes\n",
    "        # not depend on global variables.\n",
    "        self.dir_input = dir_input\n",
    "        self.dir_truth = dir_truth\n",
    "        self.sample_size = sample_size\n",
    "        self.classes = classes\n",
    "\n",
    "        # Walk through the inputs directory and add each file to our items list\n",
    "        self.items = []\n",
    "        for (_, _, filenames) in os.walk(self.dir_input):\n",
    "            self.items.extend([CityscapesSample.from_filename(f) for f in filenames])\n",
    "\n",
    "        # Sanity check: do the provided directories contain any samples?\n",
    "        assert len(self.items) > 0, f\"No items found in {self.dir_input}\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, i: int) -> (torch.LongTensor, torch.LongTensor):\n",
    "        sample = self.items[i]\n",
    "\n",
    "        input = self.load_input(sample)\n",
    "        truth = self.load_truth(sample)\n",
    "\n",
    "        return self.transform(input, truth)\n",
    "\n",
    "    def load_input(self, sample: CityscapesSample) -> Image:\n",
    "        path = os.path.join(self.dir_input, f'{sample.id}_leftImg8bit.png')\n",
    "        return Image.open(path).convert(\"RGB\").resize(self.sample_size, Image.NEAREST)\n",
    "\n",
    "    def load_truth(self, sample:CityscapesSample) -> Image:\n",
    "        path = os.path.join(self.dir_truth, f'{sample.id}_gtFine_color.png')\n",
    "        return Image.open(path).convert(\"RGB\").resize(self.sample_size, Image.NEAREST)\n",
    "\n",
    "    def transform(self, img: Image.Image, mask: Optional[Image.Image]) -> (torch.LongTensor, torch.LongTensor):\n",
    "        ## EXERCISE #####################################################################\n",
    "        #\n",
    "        # Data augmentation is a way to improve the accuracy of a model.\n",
    "        #\n",
    "        # Once you have a model that works, you can implement some data augmentation \n",
    "        # techniques here to further improve performance.\n",
    "        #\n",
    "        ##################################################################################\n",
    "\n",
    "        # Random horizontal flip\n",
    "        if random.random() > 0.5:\n",
    "            img = TF.hflip(img)\n",
    "            if mask is not None:\n",
    "                mask = TF.hflip(mask)\n",
    "\n",
    "        # Random rotation\n",
    "        angle = random.uniform(-10, 10)\n",
    "        img = TF.rotate(img, angle)\n",
    "        if mask is not None:\n",
    "            mask = TF.rotate(mask, angle)\n",
    "\n",
    "        # Random color jitter\n",
    "        img = transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1)(img)\n",
    "\n",
    "        # Adjust brightness\n",
    "        brightness_factor = random.uniform(0.5, 1.5)\n",
    "        img = TF.adjust_brightness(img, brightness_factor)\n",
    "        \n",
    "        # Noise\n",
    "        if random.random() >= 0.4:\n",
    "            w, h, ch = np.asarray(img).shape\n",
    "            img = np.asarray(img)\n",
    "            gauss = np.random.normal(0, 1, (w, h, ch))*0.5\n",
    "            gauss = np.asarray(gauss)\n",
    "            img = np.add(img, gauss).astype(\"uint8\")\n",
    "\n",
    "        ################################################################################# \n",
    "\n",
    "        # Convert the image to a tensor\n",
    "        img = TF.to_tensor(img)\n",
    "\n",
    "        # If no mask is provided, then return only the image\n",
    "        if mask is None:\n",
    "            return img, None\n",
    "\n",
    "        # Transform the mask from an image with RGB-colors to an 1-channel image with the index of the class as value\n",
    "        mask_size = [s for s in self.sample_size]\n",
    "        mask = torch.from_numpy(np.array(mask)).permute((2,0,1))\n",
    "        target = torch.zeros((mask_size[1], mask_size[0]), dtype=torch.uint8)\n",
    "        for i,c in enumerate(classes):\n",
    "            eq = mask[0].eq(c.color[0]) & mask[1].eq(c.color[1]) & mask[2].eq(c.color[2])\n",
    "            target += eq * c.trainId           \n",
    "            \n",
    "        target[target>200] = 255\n",
    "\n",
    "        img = torch.nn.functional.normalize(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def masks_to_indices(self, masks: torch.Tensor) -> torch.Tensor:\n",
    "        _, indices = masks.softmax(dim=1).max(dim=1)\n",
    "        return indices\n",
    "\n",
    "    def snow(self, img, target):\n",
    "        sidewalk = (target==1)\n",
    "        scaler = np.random.randint(800, 1000, size=(sidewalk.shape))/1000\n",
    "        sidewalk *=scaler\n",
    "        for i in range(3):\n",
    "            img[i,:,:] = torch.max(sidewalk,img[i,:,:])\n",
    "        \"\"\"Tensor2PIL = transforms.ToPILImage()\n",
    "        Img = Tensor2PIL(img)\n",
    "        display(Img)\"\"\"\n",
    "        return img\n",
    "\n",
    "    def to_image(self, indices: torch.Tensor) -> Image.Image:\n",
    "        target = torch.zeros((3, indices.shape[0], indices.shape[1]),\n",
    "                             dtype=torch.uint8, device=indices.device, requires_grad=False)\n",
    "\n",
    "        for i, lbl in enumerate(self.classes):\n",
    "            eq = indices.eq(lbl.trainId)\n",
    "\n",
    "            target[0] += eq * lbl.color[0]\n",
    "            target[1] += eq * lbl.color[1]\n",
    "            target[2] += eq * lbl.color[2]\n",
    "\n",
    "        return TF.to_pil_image(target.cpu(), 'RGB')\n",
    "\n",
    "\n",
    "# Create one instance of the CityscapesDataset for each split type\n",
    "ds_split = {\n",
    "    name:CityscapesDataset(os.path.join(dir_input_pp, name), os.path.join(dir_truth_pp, name), sample_size, classes)\n",
    "    for name in (\"train\", \"val\", \"test\")\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "valid-spare",
   "metadata": {},
   "source": [
    "Random Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-newman",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from io import BytesIO\n",
    "from base64 import b64encode\n",
    "\n",
    "import random\n",
    "\n",
    "# HTML templates for displaying random samples in a table\n",
    "template_table = '<table><thead><tr><th>Subset</th><th>Amount</th><th>Size</th><th>Input sample</th><th>Truth sample</th></tr></thead><tbody>{0}</tbody></table>'\n",
    "template_row = '<tr><td>{0}</td><td>{1}</td><td>{2}</td><td>{3}</td><td>{4}</td></tr>'\n",
    "template_img = '<img src=\"data:image/png;base64,{0}\"/>'\n",
    "\n",
    "# Display a random sample of each split of the dataset\n",
    "rows = []\n",
    "for name, ds_sub in ds_split.items():\n",
    "    # Draw a random sample from the dataset so that we can convert it back to an image\n",
    "    input, truth = random.choice(ds_sub)\n",
    "    #print(torch.unique(truth))\n",
    "\n",
    "    input = TF.to_pil_image(input)\n",
    "    truth = ds_sub.to_image(truth)\n",
    "\n",
    "    # Create a buffer to save each retrieved image into such that we can base64-encode it for diplay in our HTML table\n",
    "    with BytesIO() as buffer_input, BytesIO() as buffer_truth:\n",
    "        input.save(buffer_input, format='png')\n",
    "        truth.save(buffer_truth, format='png')\n",
    "\n",
    "        # Store one row of the dataset\n",
    "        images = [template_img.format(b64encode(b.getvalue()).decode('utf-8')) for b in (buffer_input, buffer_truth)]\n",
    "        rows.append(template_row.format(name, len(ds_sub), '&times;'.join([str(s) for s in input.size]), *images))\n",
    "\n",
    "# Render HTML table\n",
    "table = template_table.format(''.join(rows))\n",
    "display(HTML(table))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "specialized-spoke",
   "metadata": {},
   "source": [
    "## Calculating accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-sapphire",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(output: torch.Tensor, truths: torch.Tensor) -> float:\n",
    "    output = output.detach().cpu()  \n",
    "    truths = truths.detach().cpu()\n",
    "\n",
    "    ## EXERCISE #####################################################################\n",
    "    #\n",
    "    # Implement the IoU metric that is used by the benchmark to grade your results.\n",
    "    #     \n",
    "    # `output` is a tensor of dimensions [Batch, Classes, Height, Width]\n",
    "    # `truths` is a tensor of dimensions [Batch, Height, Width]\n",
    "    #\n",
    "    # Tip: Peform a sanity check that tests your implementation on a user-defined \n",
    "    #      tensor for which you know what the output should be.\n",
    "    #\n",
    "    ################################################################################# \n",
    "\n",
    "    num_batch, num_class, _, _ = output.size()\n",
    "    IOU = []\n",
    "    for i in range(num_batch):\n",
    "        class_id = torch.max(output[i],dim = 0).indices\n",
    "        intersection = []\n",
    "        union = []\n",
    "        for j in range(num_class):\n",
    "            pred = (class_id == j)\n",
    "            true = (truths[i] == j)\n",
    "            intersection.append((pred & true).sum())\n",
    "            union.append((pred | true).sum())\n",
    "        IOU.append(sum(intersection)/(sum(union) + 1e-5))\n",
    "    iou = sum(IOU)/num_batch\n",
    "    #################################################################################\n",
    "\n",
    "    return iou\n",
    "\n",
    "def compute_class_dice(output: torch.Tensor, truths: torch.Tensor) -> float:\n",
    "    output = output.detach().cpu()  \n",
    "    truths = truths.detach().cpu()\n",
    "    \n",
    "    num_batch, num_class, _, _ = output.size()\n",
    "    \n",
    "    dice = []\n",
    "    for b in range(num_batch):\n",
    "        class_id = torch.max(output[b],dim = 0).indices\n",
    "        dice_class = []\n",
    "        for c in range(num_class):\n",
    "            pred = torch.sum((class_id == c), (0,1))\n",
    "            true = torch.sum((truths[b] == c), (0,1))\n",
    "            intersection = torch.sum((pred & true))\n",
    "            dice_class.append(2*intersection/(pred + true + 1e-5))\n",
    "\n",
    "        dice.append(np.mean(dice_class))\n",
    "\n",
    "    dices = np.mean(dice)\n",
    "\n",
    "    return dices            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "unique-university",
   "metadata": {},
   "source": [
    "## Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-google",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model: nn.Module, ds_split: Dict[str,CityscapesDataset]):\n",
    "        # Choose a device to run training on. Ideally, you have a GPU available to accelerate the training process.\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(self.device)\n",
    "\n",
    "        # Move the model onto the target device\n",
    "        self.model = model.to(self.device)\n",
    "\n",
    "        # Store the dataset split\n",
    "        self.ds_split = ds_split\n",
    "\n",
    "        ## EXERCISE #####################################################################\n",
    "        #\n",
    "        # Select an optimizer\n",
    "        #\n",
    "        # See: https://pytorch.org/docs/stable/optim.html\n",
    "        #\n",
    "        ################################################################################# \n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters())\n",
    "\n",
    "        ## EXERCISE #####################################################################\n",
    "        #\n",
    "        # Select an appropriate loss function\n",
    "        #\n",
    "        # See: https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "        #\n",
    "        ################################################################################# \n",
    "\n",
    "        self.critereon = nn.CrossEntropyLoss(ignore_index=255)\n",
    "\n",
    "        ################################################################################# \n",
    "\n",
    "        assert self.critereon is not None, \"You have not defined a loss\"\n",
    "        assert self.optimizer is not None, \"You have not defined an optimizer\"\n",
    "\n",
    "    def train_epoch(self, dl:DataLoader):\n",
    "        # Put the model in training mode\n",
    "        self.model.train()\n",
    "\n",
    "        # Store each step's accuracy and loss for this epoch\n",
    "        epoch_metrics = {\n",
    "            \"loss\": [],\n",
    "            \"accuracy\": []\n",
    "        }\n",
    "\n",
    "        # Create a progress bar using TQDM\n",
    "        sys.stdout.flush()\n",
    "        with tqdm(total=len(self.ds_split[\"train\"]), desc=f'Training') as pbar:\n",
    "            # Iterate over the training dataset\n",
    "            for inputs, truths in dl:\n",
    "                # Zero the gradients from the previous step\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # Move the inputs and truths to the target device\n",
    "                inputs = inputs.to(device=self.device, dtype=torch.float32)\n",
    "                inputs.required_grad = True  # Fix for older PyTorch versions\n",
    "                truths = truths.to(device=self.device, dtype=torch.long)\n",
    "\n",
    "                # Run model on the inputs\n",
    "                output = self.model(inputs)\n",
    "\n",
    "                # Perform backpropagation\n",
    "                loss = self.critereon(output, truths)\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_value_(self.model.parameters(), 0.1)\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Store the metrics of this step\n",
    "                step_metrics = {\n",
    "                    'loss': loss.item(),\n",
    "                    'accuracy': compute_iou(output, truths)\n",
    "                }\n",
    "\n",
    "                # Update the progress bar\n",
    "                pbar.set_postfix(**step_metrics)\n",
    "                pbar.update(list(inputs.shape)[0])\n",
    "\n",
    "                # Add to epoch's metrics\n",
    "                for k,v in step_metrics.items():\n",
    "                    epoch_metrics[k].append(v)\n",
    "\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Return metrics\n",
    "        return epoch_metrics\n",
    "\n",
    "    def val_epoch(self, dl:DataLoader):\n",
    "        # Put the model in evaluation mode\n",
    "        self.model.eval()\n",
    "\n",
    "        # Store the total loss and accuracy over the epoch\n",
    "        amount = 0\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "\n",
    "        # Create a progress bar using TQDM\n",
    "        sys.stdout.flush()\n",
    "        with torch.no_grad(), tqdm(total=len(self.ds_split[\"val\"]), desc=f'Validation') as pbar:\n",
    "            # Iterate over the validation dataloader\n",
    "            for inputs, truths in dl:\n",
    "                 # Move the inputs and truths to the target device\n",
    "                inputs = inputs.to(device=self.device, dtype=torch.float32)\n",
    "                inputs.required_grad = True  # Fix for older PyTorch versions\n",
    "                truths = truths.to(device=self.device, dtype=torch.long)\n",
    "\n",
    "                # Run model on the inputs\n",
    "                output = self.model(inputs)\n",
    "                loss = self.critereon(output, truths)\n",
    "\n",
    "                # Store the metrics of this step\n",
    "                step_metrics = {\n",
    "                    'loss': loss.item(),\n",
    "                    'accuracy': compute_iou(output, truths)\n",
    "                }\n",
    "\n",
    "                # Update the progress bar\n",
    "                pbar.set_postfix(**step_metrics)\n",
    "                pbar.update(list(inputs.shape)[0])\n",
    "\n",
    "                amount += 1\n",
    "                total_loss += step_metrics[\"loss\"]\n",
    "                total_accuracy += step_metrics[\"accuracy\"]\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # Print mean of metrics\n",
    "        total_loss /= amount\n",
    "        total_accuracy /= amount\n",
    "        print(f'Validation loss is {total_loss/amount}, validation accuracy is {total_accuracy}')\n",
    "\n",
    "        # Return mean loss and accuracy\n",
    "        return {\n",
    "            \"loss\": [total_loss],\n",
    "            \"accuracy\": [total_accuracy]\n",
    "        }\n",
    "\n",
    "\n",
    "    def fit(self, epochs: int, batch_size:int):\n",
    "        # Initialize Dataloaders for the `train` and `val` splits of the dataset. \n",
    "        # A Dataloader loads a batch of samples from the each dataset split and concatenates these samples into a batch.\n",
    "        dl_train = DataLoader(ds_split[\"train\"], batch_size=batch_size, shuffle=True)\n",
    "        dl_val = DataLoader(ds_split[\"val\"], batch_size=batch_size, drop_last=True)\n",
    "\n",
    "        # Store metrics of the training process (plot this to gain insight)\n",
    "        df_train = pd.DataFrame()\n",
    "        df_val = pd.DataFrame()\n",
    "\n",
    "        # Train the model for the provided amount of epochs\n",
    "        for epoch in range(1, epochs+1):\n",
    "            print(f'Epoch {epoch}')\n",
    "            metrics_train = self.train_epoch(dl_train)\n",
    "            df_train = df_train.append(pd.DataFrame({'epoch': [epoch for _ in range(len(metrics_train[\"loss\"]))], **metrics_train}), ignore_index=True)\n",
    "\n",
    "            metrics_val = self.val_epoch(dl_val)\n",
    "            df_val = df_val.append(pd.DataFrame({'epoch': [epoch], **metrics_val}), ignore_index=True)\n",
    "\n",
    "        # Return a dataframe that logs the training process. This can be exported to a CSV or plotted directly.\n",
    "        return df_train, df_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f6f38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# # Define a module that transforms: RGB-channel image -> len(classes)-channel image\n",
    "# class Passthrough(nn.Module):\n",
    "#     def __init__(self, n_channels=3, n_classes=len(classes)):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.transform = nn.Conv2d(n_channels, n_classes, 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.softmax(self.transform(x), dim=1)\n",
    "#         return x\n",
    "\n",
    "# model = Passthrough()\n",
    "    \n",
    "\n",
    "# # Train the passthrough network\n",
    "# print(\"Testing training process...\")\n",
    "# trainer = Trainer(model, ds_split)\n",
    "# trainer.fit(epochs=1, batch_size=10)\n",
    "\n",
    "# # Draw a random sample\n",
    "# input, truth = random.choice(ds_split[\"val\"])\n",
    "\n",
    "# # Push through our network\n",
    "# model = model.cpu()\n",
    "# output = model(input.unsqueeze(0))\n",
    "\n",
    "# # Display the input, output and truth tensors\n",
    "# template_table = '<table><thead><tr><th>Tensor</th><th>Shape</th><th>Image</th></tr></thead><tbody>{0}</tbody></table>'\n",
    "# template_row = '<tr><td>{0}</td><td>{1}</td><td><img src=\"data:image/png;base64,{2}\"/></td></tr>'\n",
    "\n",
    "# input_img = TF.to_pil_image(input)\n",
    "# output_img = ds_split[\"val\"].to_image(ds_split[\"val\"].masks_to_indices(output).squeeze(0))\n",
    "# truth_img = ds_split[\"val\"].to_image(truth)\n",
    "\n",
    "# rows = []\n",
    "# for name, tensor, img in [('Input', input, input_img), ('Output', output, output_img), ('Target', truth, truth_img)]:\n",
    "#     with BytesIO() as b:\n",
    "#         img.save(b, format='png')\n",
    "#         rows.append(template_row.format(name, list(tensor.shape), b64encode(b.getvalue()).decode('utf-8')))\n",
    "\n",
    "# # Render HTML table\n",
    "# table = template_table.format(''.join(rows))\n",
    "# display(HTML(table))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "tracked-liverpool",
   "metadata": {},
   "source": [
    "## Implementation of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-stanley",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a baseline from literature\n",
    "class conv(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1)\n",
    "        self.batchN1 = nn.BatchNorm2d(out_channel)\n",
    "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1)\n",
    "        self.batchN2 = nn.BatchNorm2d(out_channel)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.batchN1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batchN2(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class encoder(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        self.conv = conv(in_channel, out_channel)\n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "    def forward(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        p = self.pool(x)\n",
    "        return x, p\n",
    "    \n",
    "class decoder(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channel, out_channel, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = conv(out_channel + out_channel, out_channel)\n",
    "    def forward(self, inputs, skip):\n",
    "        x = self.up(inputs)\n",
    "        x = torch.cat([x, skip], axis=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class unet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        self.enc1 = encoder(3, 32)\n",
    "        self.enc2 = encoder(32, 64)\n",
    "        self.enc3 = encoder(64, 128)\n",
    "        self.enc4 = encoder(128, 256)\n",
    "        self.enc5 = encoder(256, 512)\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        self.bn = conv(512, 1024)\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        self.dec1 = decoder(1024, 512)\n",
    "        self.dec2 = decoder(512, 256)\n",
    "        self.dec3 = decoder(256, 128)\n",
    "        self.dec4 = decoder(128, 64)\n",
    "        self.dec5 = decoder(64, 32)\n",
    "        \"\"\" Classifier \"\"\"\n",
    "        self.outputs = nn.Conv2d(32, len(classes), kernel_size=1, padding=0)\n",
    "    def forward(self, inputs):\n",
    "        \"\"\" Encoder \"\"\"\n",
    "        skipC1, x1 = self.enc1(inputs)\n",
    "        skipC2, x2 = self.enc2(x1)\n",
    "        skipC3, x3 = self.enc3(x2)\n",
    "        skipC4, x4 = self.enc4(x3)\n",
    "        skipC5, x5 = self.enc5(x4)\n",
    "        \"\"\" Bottleneck \"\"\"\n",
    "        b = self.bn(x5)\n",
    "        \"\"\" Decoder \"\"\"\n",
    "        y1 = self.dec1(b, skipC5)\n",
    "        y2 = self.dec2(y1, skipC4)\n",
    "        y3 = self.dec3(y2, skipC3)\n",
    "        y4 = self.dec4(y3, skipC2)\n",
    "        y5 = self.dec5(y4, skipC1)\n",
    "        \"\"\" Classifier \"\"\"\n",
    "        outputs = self.outputs(y5)\n",
    "        return outputs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ef7e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(torch.nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, rate=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(planes)\n",
    "        self.conv2 = torch.nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               dilation=rate, padding=rate, bias=False)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(planes)\n",
    "        self.conv3 = torch.nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = torch.nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = torch.nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.rate = rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(torch.nn.Module):\n",
    "    # model = ResNet(nInputChannels, Bottleneck, [3, 4, 23, 3], os, pretrained) #=pretrained)\n",
    "    def __init__(self, nInputChannels, block, layers, os=16, pretrained=False):\n",
    "        self.inplanes = 16\n",
    "        super(ResNet, self).__init__()\n",
    "        if os == 16:\n",
    "            strides = [1, 2, 2, 1]\n",
    "            rates = [1, 1, 1, 2]\n",
    "            blocks = [1, 2, 4]\n",
    "        elif os == 8:\n",
    "            strides = [1, 2, 1, 1]\n",
    "            rates = [1, 1, 2, 2]\n",
    "            blocks = [1, 2, 1]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Modules\n",
    "        self.conv1 = torch.nn.Conv2d(nInputChannels, 16, kernel_size=7, stride=2, padding=3,\n",
    "                                bias=False)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(16)\n",
    "        self.relu = torch.nn.ReLU(inplace=True)\n",
    "        self.maxpool = torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 16, layers[0], stride=strides[0], rate=rates[0])\n",
    "        self.layer2 = self._make_layer(block, 32, layers[1], stride=strides[1], rate=rates[1])\n",
    "        self.layer3 = self._make_layer(block, 64, layers[2], stride=strides[2], rate=rates[2])\n",
    "        self.layer4 = self._make_MG_unit(block, 128, blocks=blocks, stride=strides[3], rate=rates[3])\n",
    "\n",
    "        #self._init_weight()\n",
    "\n",
    "        if pretrained:\n",
    "            self._load_pretrained_model()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, rate=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                torch.nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, rate, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return torch.nn.Sequential(*layers)\n",
    "\n",
    "    def _make_MG_unit(self, block, planes, blocks=[1,2,4], stride=1, rate=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                torch.nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, rate=blocks[0]*rate, downsample=downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, len(blocks)):\n",
    "            layers.append(block(self.inplanes, planes, stride=1, rate=blocks[i]*rate))\n",
    "\n",
    "        return torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        low_level_feat = x\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x, low_level_feat\n",
    "\n",
    "    def _load_pretrained_model(self):\n",
    "        pretrain_dict = model_zoo.load_url('https://download.pytorch.org/models/resnet101-5d3b4d8f.pth')\n",
    "        model_dict = {}\n",
    "        state_dict = self.state_dict()\n",
    "        for k, v in pretrain_dict.items():\n",
    "            if k in state_dict:\n",
    "                model_dict[k] = v\n",
    "        state_dict.update(model_dict)\n",
    "        self.load_state_dict(state_dict)\n",
    "\n",
    "def ResNet101(nInputChannels=3, os=16, pretrained=False):\n",
    "    model_resnet = ResNet(nInputChannels, Bottleneck, [3, 4, 23, 3], os, pretrained) #=pretrained)\n",
    "    return model_resnet\n",
    "\n",
    "\n",
    "class ASPP_module(torch.nn.Module):\n",
    "    def __init__(self, inplanes, planes, rate):\n",
    "        super(ASPP_module, self).__init__()\n",
    "        if rate == 1:\n",
    "            kernel_size = 1\n",
    "            padding = 0\n",
    "        else:\n",
    "            kernel_size = 3\n",
    "            padding = rate\n",
    "        self.atrous_convolution = torch.nn.Conv2d(inplanes, planes, kernel_size=kernel_size,\n",
    "                                            stride=1, padding=padding, dilation=rate, bias=False)\n",
    "        self.bn = torch.nn.BatchNorm2d(planes)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "       # self._init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.atrous_convolution(x)\n",
    "        x = self.bn(x)\n",
    "\n",
    "        return self.relu(x)\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                # m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, torch.nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "class DeepLabv3_plus(torch.nn.Module):\n",
    "    def __init__(self, nInputChannels=3, n_classes=19, os=16, pretrained=False, _print=False):\n",
    "        if _print:\n",
    "            print(\"Constructing DeepLabv3+ model...\")\n",
    "            print(\"Number of classes: {}\".format(n_classes))\n",
    "            print(\"Output stride: {}\".format(os))\n",
    "            print(\"Number of Input Channels: {}\".format(nInputChannels))\n",
    "        super(DeepLabv3_plus, self).__init__()\n",
    "\n",
    "        # Atrous Conv\n",
    "        self.resnet_features = ResNet101(nInputChannels, os, pretrained) #=pretrained)\n",
    "\n",
    "        # ASPP\n",
    "        if os == 16: ## output_stride\n",
    "            rates = [1, 6, 12, 18]\n",
    "        elif os == 8: ## output_stride\n",
    "            rates = [1, 12, 24, 36]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.aspp1 = ASPP_module(512, 64, rate=rates[0])\n",
    "        self.aspp2 = ASPP_module(512, 64, rate=rates[1])\n",
    "        self.aspp3 = ASPP_module(512, 64, rate=rates[2])\n",
    "        self.aspp4 = ASPP_module(512, 64, rate=rates[3])\n",
    "\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "        self.global_avg_pool = torch.nn.Sequential(torch.nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                                             torch.nn.Conv2d(512, 64, 1, stride=1, bias=False),\n",
    "                                             torch.nn.BatchNorm2d(64),\n",
    "                                             torch.nn.ReLU())\n",
    "\n",
    "        self.conv1 = torch.nn.Conv2d(320, 64, 1, bias=False)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(64)\n",
    "\n",
    "        # adopt [1x1, 48] for channel reduction.\n",
    "        self.conv2 = torch.nn.Conv2d(64, 48, 1, bias=False)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(48)\n",
    "\n",
    "        self.last_conv = torch.nn.Sequential(torch.nn.Conv2d(112, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                       torch.nn.BatchNorm2d(64),\n",
    "                                       torch.nn.ReLU(),\n",
    "                                       torch.nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                                       torch.nn.BatchNorm2d(64),\n",
    "                                       torch.nn.ReLU(),\n",
    "                                       torch.nn.Conv2d(64, n_classes, kernel_size=1, stride=1))\n",
    "\n",
    "    def forward(self, input):\n",
    "        x, low_level_features = self.resnet_features(input)\n",
    "        x1 = self.aspp1(x)\n",
    "        x2 = self.aspp2(x)\n",
    "        x3 = self.aspp3(x)\n",
    "        x4 = self.aspp4(x)\n",
    "        x5 = self.global_avg_pool(x)\n",
    "        x5 = F.interpolate(x5, size=x4.size()[2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = F.interpolate(x, size=(int(math.ceil(input.size()[-2]/4)),\n",
    "                                int(math.ceil(input.size()[-1]/4))), mode='bilinear', align_corners=True)\n",
    "\n",
    "        low_level_features = self.conv2(low_level_features)\n",
    "        low_level_features = self.bn2(low_level_features)\n",
    "        low_level_features = self.relu(low_level_features)\n",
    "\n",
    "\n",
    "        x = torch.cat((x, low_level_features), dim=1)\n",
    "        #print(\"last conv\", x.shape)\n",
    "        x = self.last_conv(x)\n",
    "        x = F.interpolate(x, size=input.size()[2:], mode='bilinear', align_corners=True)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def freeze_bn(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.BatchNorm2d):\n",
    "                m.eval()\n",
    "\n",
    "    def __init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                # n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                # m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, torch.nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95757b12",
   "metadata": {},
   "source": [
    "## Train model and save acc&loss to csv.file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c9df39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing training process...\")\n",
    "model = DeepLabv3_plus()\n",
    "trainer = Trainer(model, ds_split)\n",
    "df_train, df_val = trainer.fit(epochs=10, batch_size=10)\n",
    "\n",
    "# add a column to indicate the dataset (train or val)\n",
    "df_train[\"dataset\"] = \"train\"\n",
    "df_val[\"dataset\"] = \"val\"\n",
    "\n",
    "# concatenate the dataframes and save to a csv file\n",
    "df_concat = pd.concat([df_train, df_val])\n",
    "df_concat.to_csv(\"deeplabv3.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28597097",
   "metadata": {},
   "source": [
    "## Randomly plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa41a8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Draw a random sample\n",
    "input, truth = random.choice(ds_split[\"val\"])\n",
    "# Push through our network\n",
    "model = model.cuda()\n",
    "output = model(input.unsqueeze(0))\n",
    "\n",
    "# Display the input, output and truth tensors\n",
    "template_table = '<table><thead><tr><th>Tensor</th><th>Shape</th><th>Image</th></tr></thead><tbody>{0}</tbody></table>'\n",
    "template_row = '<tr><td>{0}</td><td>{1}</td><td><img src=\"data:image/png;base64,{2}\"/></td></tr>'\n",
    "\n",
    "input_img = TF.to_pil_image(input)\n",
    "output_img = ds_split[\"val\"].to_image(ds_split[\"val\"].masks_to_indices(output).squeeze(0))\n",
    "truth_img = ds_split[\"val\"].to_image(truth)\n",
    "\n",
    "rows = []\n",
    "for name, tensor, img in [('Input', input, input_img), ('Output', output, output_img), ('Target', truth, truth_img)]:\n",
    "    with BytesIO() as b:\n",
    "        img.save(b, format='png')\n",
    "        rows.append(template_row.format(name, list(tensor.shape), b64encode(b.getvalue()).decode('utf-8')))\n",
    "\n",
    "# Render HTML table\n",
    "table = template_table.format(''.join(rows))\n",
    "display(HTML(table))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad496cdb",
   "metadata": {},
   "source": [
    "## Read csv file and plot the result(Acc & loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273fa316",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# read in the csv file as a pandas dataframe\n",
    "df = pd.read_csv('train_val_metrics3.csv', dtype={'accuracy': 'str', 'loss': 'str'})\n",
    "\n",
    "# remove the 'tensor(' and ')' from the 'accuracy' and 'loss' columns and convert them to floats\n",
    "df['accuracy'] = df['accuracy'].apply(lambda x: float(re.findall(\"\\d+\\.\\d+\", x)[0]))\n",
    "df['loss'] = df['loss'].apply(lambda x: float(re.findall(\"\\d+\\.\\d+\", x)[0]))\n",
    "\n",
    "# separate the training and validation data\n",
    "df_train = df[df[\"dataset\"] == \"train\"]\n",
    "df_val = df[df[\"dataset\"] == \"val\"]\n",
    "\n",
    "# calculate the average loss and accuracy for each epoch for the training set\n",
    "train_epoch_loss = df_train.groupby(\"epoch\")[\"loss\"].mean()\n",
    "train_epoch_acc = df_train.groupby(\"epoch\")[\"accuracy\"].mean()\n",
    "\n",
    "# extract the validation data for each epoch\n",
    "val_epoch_loss = df_val.groupby(\"epoch\")[\"loss\"].unique().apply(lambda x: x[0])\n",
    "val_epoch_acc = df_val.groupby(\"epoch\")[\"accuracy\"].unique().apply(lambda x: x[0])\n",
    "\n",
    "# plot the data\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax1.plot(train_epoch_loss, label=\"Training Loss\")\n",
    "ax1.plot(val_epoch_loss, label=\"Validation Loss\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_title(\"Training and Validation Loss\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(train_epoch_acc, label=\"Training Accuracy\")\n",
    "ax2.plot(val_epoch_acc, label=\"Validation Accuracy\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Accuracy\")\n",
    "ax2.set_title(\"Training and Validation Accuracy\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
